Project plan

1. Scrap the site and collect the data into json: https://raw.githubusercontent.com/rzabolotin/nz-visa-assistant/refs/heads/main/data/site_content.json
2. Create ground-truth.json with potentail questions and url - for evaluation retrieval

- use VS Code Cline AI assistnat to build python code from Jupyter notebooks

I did setx OPENAI_API_KEY YOUR_API_KEY and got SUCCESS: Specified value was saved.

Prompt:
"""
You are python developer, creating and building python project named "Prague events Chat": end-to-end RAG application.
Chat should get user questions and use RAG to reply, based on the context knowledge base. Question example: "What is going in Prague cinemas?"

For that, you need:
- Context knowledge base dataset in russian. Use files from Project/data/vinegret_articles/json/*.json. Use data from "paragraphs" block
- Ingest the data into a knowledge base. Use ideas from the file homework1.ipynb. 
- Implement the retrieval flow: query the knowledge base, build the prompt, send the promt to an LLM. Use ideas from the file homework1.ipynb. Use OpenAI for LLM. I will provide OPENAI_API_KEY, you can use it. 
- Evaluate the performance of your RAG flow. Use hit_rate() and mrr() from the file homework3.ipynb. 
- Create an interface for the application
- Collect user feedback and monitor your application
- Use this project as a rule of thumb and basis https://github.com/alexkolo/rag_nutrition_facts_blog/. 

LLM: OpenAI
Knowledge base: vector database Qdrant
Monitoring: Grafana, Kibana, Streamlit, dash, etc
Interface: Streamlit, FastAPI
Ingestion pipeline: Mage, dlt, Airflow, Prefect, python script, etc
Container: Docker

Evaluation Criteria:
Problem description - The problem is well-described and it's clear what problem the project solves
Retrieval flow - Both a knowledge base and an LLM are used in the flow
Retrieval evaluation - Multiple retrieval approaches are evaluated, and the best one is used
LLM evaluation - Multiple approaches are evaluated, and the best one is used
Interface - UI (e.g., Streamlit), web application (e.g., Django), or an API (e.g., built with FastAPI)
Ingestion pipeline - Automated ingestion with a Python script or a special tool (e.g., Mage, dlt, Airflow, Prefect)
Monitoring - User feedback is collected and there's a dashboard with at least 5 charts
Containerization - Everything is in docker-compose
Reproducibility - Instructions are clear, the dataset is accessible, it's easy to run the code, and it works. The versions for all dependencies are specified.
Best practices - User query rewriting 
"""


To run the application:
1. Set your OPENAI_API_KEY environment variable.
2. Open Docker Desktop 
3. <For local run> Ensure your VS Code terminal folder is C:\Users\vpere\Desktop\Git\LLMZoomCamp\Project
4. Run in terminal `docker-compose up --build` to start Qdrant and the Streamlit app. Ensure Containers "qdrant" and "prague_events_chat_app" are running.
4.1 In a separate terminal, run 
	python ./scripts/web_scraping.py (~40mins) - crawl a website, extract blog article URLs, translate and save the content and metadata of those articles into JSONs 
4.2 In a separate terminal, run the ingestion script inside the app container - to ingest data into Qdrant:
	docker exec -it prague_events_chat_app python scripts/ingest_vinegret_to_qdrant.py
5. Access the Streamlit app at http://localhost:8501 to interact with the Prague events Chat.


To release the space in Dcoker:
1. Remove unused Docker data (containers, images, networks, volumes):
	docker system prune -a
2. Clean up Docker volumes separately (if needed):
	docker volume prune


Applcation description:
 - web_scraping,py - This script is a web scraper designed to crawl a website, extract blog article URLs, and save the content and metadata of those articles into JSON files. Here's a breakdown of the key components:m

	

